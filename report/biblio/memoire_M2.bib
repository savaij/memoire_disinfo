@misc{arevalo2017,
  title = {Gated {{Multimodal Units}} for {{Information Fusion}}},
  author = {Arevalo, John and Solorio, Thamar and {Montes-y-Gómez}, Manuel and González, Fabio A.},
  year = {2017},
  month = feb,
  number = {arXiv:1702.01992},
  eprint = {1702.01992},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.01992},
  url = {http://arxiv.org/abs/1702.01992},
  urldate = {2025-08-13},
  abstract = {This paper presents a novel model for multimodal learning based on gated neural networks. The Gated Multimodal Unit (GMU) model is intended to be used as an internal unit in a neural network architecture whose purpose is to find an intermediate representation based on a combination of data from different modalities. The GMU learns to decide how modalities influence the activation of the unit using multiplicative gates. It was evaluated on a multilabel scenario for genre classification of movies using the plot and the poster. The GMU improved the macro f-score performance of single-modality approaches and outperformed other fusion strategies, including mixture of experts models. Along with this work, the MM-IMDb dataset is released which, to the best of our knowledge, is the largest publicly available multimodal dataset for genre prediction on movies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/sava/Zotero/storage/5SQU3ZL3/Arevalo et al. - 2017 - Gated Multimodal Units for Information Fusion.pdf;/Users/sava/Zotero/storage/JTFNQE5F/1702.html}
}

@misc{BBC-2025,
  title = {Fact-checking {{Donald Trump}}'s {{Oval Office}} confrontation with {{Cyril Ramaphosa}}},
  year = {2025},
  month = may,
  url = {https://www.bbc.com/news/articles/ce9vxve994ro},
  urldate = {2025-08-02},
  abstract = {President Trump made a series of claims about the killing of white farmers in South Africa, some of which are demonstrably false.},
  langid = {british},
  file = {/Users/sava/Zotero/storage/AYBJAWSZ/ce9vxve994ro.html}
}

@inproceedings{cao2017,
  title = {{{DeepHawkes}}: {{Bridging}} the {{Gap}} between {{Prediction}} and {{Understanding}} of {{Information Cascades}}},
  shorttitle = {{{DeepHawkes}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Cao, Qi and Shen, Huawei and Cen, Keting and Ouyang, Wentao and Cheng, Xueqi},
  year = {2017},
  month = nov,
  series = {{{CIKM}} '17},
  pages = {1149--1158},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3132847.3132973},
  url = {https://doi.org/10.1145/3132847.3132973},
  urldate = {2025-08-03},
  abstract = {Online social media remarkably facilitates the production and delivery of information, intensifying the competition among vast information for users' attention and highlighting the importance of predicting the popularity of information. Existing approaches for popularity prediction fall into two paradigms: feature-based approaches and generative approaches. Feature-based approaches extract various features (e.g., user, content, structural, and temporal features), and predict the future popularity of information by training a regression/classification model. Their predictive performance heavily depends on the quality of hand-crafted features. In contrast, generative approaches devote to characterizing and modeling the process that a piece of information accrues attentions, offering us high ease to understand the underlying mechanisms governing the popularity dynamics of information cascades. But they have less desirable predictive power since they are not optimized for popularity prediction. In this paper, we propose DeepHawkes to combat the defects of existing methods, leveraging end-to-end deep learning to make an analogy to interpretable factors of Hawkes process --- a widely-used generative process to model information cascade. DeepHawkes inherits the high interpretability of Hawkes process and possesses the high predictive power of deep learning methods, bridging the gap between prediction and understanding of information cascades. We verify the effectiveness of DeepHawkes by applying it to predict retweet cascades of Sina Weibo and citation cascades of a longitudinal citation dataset. Experimental results demonstrate that DeepHawkes outperforms both feature-based and generative approaches.},
  isbn = {978-1-4503-4918-5}
}

@inproceedings{devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423/},
  urldate = {2025-08-15},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  file = {/Users/sava/Zotero/storage/JZPIJ87T/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf}
}

@misc{ipsos2023,
  title = {{{UNESCO Study}} on the impact of online disinformation during election campaigns},
  author = {{Ipsos}},
  year = {2023},
  url = {https://www.unesco.org/sites/default/files/medias/fichiers/2023/11/unesco_ipsos_survey.pdf}
}

@article{kaliyar2021,
  title = {{{FakeBERT}}: {{Fake}} news detection in social media with a {{BERT-based}} deep learning approach},
  shorttitle = {{{FakeBERT}}},
  author = {Kaliyar, Rohit Kumar and Goswami, Anurag and Narang, Pratik},
  year = {2021},
  month = mar,
  journal = {Multimedia Tools and Applications},
  volume = {80},
  number = {8},
  pages = {11765--11788},
  issn = {1573-7721},
  doi = {10.1007/s11042-020-10183-2},
  url = {https://doi.org/10.1007/s11042-020-10183-2},
  urldate = {2025-08-02},
  abstract = {In the modern era of computing, the news ecosystem has transformed from old traditional print media to social media outlets. Social media platforms allow us to consume news much faster, with less restricted editing results in the spread of fake news at an incredible pace and scale. In recent researches, many useful methods for fake news detection employ sequential neural networks to encode news content and social context-level information where the text sequence was analyzed in a unidirectional way. Therefore, a bidirectional training approach is a priority for modelling the relevant information of fake news that is capable of improving the classification performance with the ability to capture semantic and long-distance dependencies in sentences. In this paper, we propose a BERT-based (Bidirectional Encoder Representations from Transformers) deep learning approach (FakeBERT) by combining different parallel blocks of the single-layer deep Convolutional Neural Network (CNN) having different kernel sizes and filters with the BERT. Such a combination is useful to handle ambiguity, which is the greatest challenge to natural language understanding. Classification results demonstrate that our proposed model (FakeBERT) outperforms the existing models with an accuracy of 98.90\%.},
  langid = {english},
  keywords = {Artificial Intelligence,BERT,Deep learning,Digital Journalism,Fake news,Machine Learning,Neural decoding,Neural encoding,Neural network,Social media,Symbolic AI},
  file = {/Users/sava/Zotero/storage/NBQZM6AA/Kaliyar et al. - 2021 - FakeBERT Fake news detection in social media with a BERT-based deep learning approach.pdf}
}

@inproceedings{krstovski2022,
  title = {Evons: {{A Dataset}} for {{Fake}} and {{Real News Virality Analysis}} and {{Prediction}}},
  shorttitle = {Evons},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Krstovski, Kriste and Ryu, Angela Soomin and Kogut, Bruce},
  editor = {Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo and Chen, Hsin-Hsi and Donatelli, Lucia and Ji, Heng and Kurohashi, Sadao and Paggio, Patrizia and Xue, Nianwen and Kim, Seokhwan and Hahm, Younggyun and He, Zhong and Lee, Tony Kyungil and Santus, Enrico and Bond, Francis and Na, Seung-Hoon},
  year = {2022},
  month = oct,
  pages = {3589--3596},
  publisher = {International Committee on Computational Linguistics},
  address = {Gyeongju, Republic of Korea},
  url = {https://aclanthology.org/2022.coling-1.317/},
  urldate = {2025-08-03},
  abstract = {We present a novel collection of news articles originating from fake and real news media sources for the analysis and prediction of news virality. Unlike existing fake news datasets which either contain claims, or news article headline and body, in this collection each article is supported with a Facebook engagement count which we consider as an indicator of the article virality. In addition we also provide the article description and thumbnail image with which the article was shared on Facebook. These images were automatically annotated with object tags and color attributes. Using cloud based vision analysis tools, thumbnail images were also analyzed for faces and detected faces were annotated with facial attributes. We empirically investigate the use of this collection on an example task of article virality prediction.},
  file = {/Users/sava/Zotero/storage/AWJ3JUSJ/Krstovski et al. - 2022 - Evons A Dataset for Fake and Real News Virality Analysis and Prediction.pdf}
}

@article{kuntur2024,
  title = {Comparative {{Analysis}} of {{Graph Neural Networks}} and {{Transformers}} for {{Robust Fake News Detection}}: {{A Verification}} and {{Reimplementation Study}}},
  shorttitle = {Comparative {{Analysis}} of {{Graph Neural Networks}} and {{Transformers}} for {{Robust Fake News Detection}}},
  author = {Kuntur, Soveatin and Krzywda, Maciej and Wróblewska, Anna and Paprzycki, Marcin and Ganzha, Maria},
  year = {2024},
  month = jan,
  journal = {Electronics},
  volume = {13},
  number = {23},
  pages = {4784},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics13234784},
  url = {https://www.mdpi.com/2079-9292/13/23/4784},
  urldate = {2025-08-02},
  abstract = {This study compares Transformer-based models and Graph Neural Networks (GNNs) for fake news detection across three datasets: FakeNewsNet, ISOT, and WELFake. Transformer models (BERT, RoBERTa, GPT-2) demonstrated superior performance, achieving mean accuracies above 85\% on FakeNewsNet and exceeding 98\% on ISOT and WELFake. Specifically, RoBERTa achieved 86.16\% accuracy on FakeNewsNet and 99.99\% on ISOT, while GPT-2 reached 99.72\% on WELFake. In contrast, GNNs (GCN, GraphSAGE, GIN, GAT) exhibited lower performance. GCN achieved 71\% accuracy on FakeNewsNet but dropped to 53.30\% on ISOT and 50.28\% on WELFake, with F1 scores reflecting similar trends. Other GNNs, like GraphSAGE, showed even lower results, particularly on ISOT and WELFake, where performance hovered around 50\%. Our findings indicate that while Transformers provide exceptional accuracy and reliability, GNNs offer potential efficiency benefits for resource-constrained scenarios despite their lower predictive performance. This study informs model selection for fake news detection tasks and encourages the exploration of hybrid approaches to balance accuracy and computational efficiency.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {comparative analysis,fake news detection,FakeNewsNet,GAT,GPT-2,graph convolutional networks (GCNs),graph neural networks (GNNs),ISOT dataset,RoBERTa,transformers},
  file = {/Users/sava/Zotero/storage/CDVVSQG4/Kuntur et al. - 2024 - Comparative Analysis of Graph Neural Networks and Transformers for Robust Fake News Detection A Ver.pdf}
}

@article{lee2023,
  title = {Mathematical {{Analysis}} and {{Performance Evaluation}} of the {{GELU Activation Function}} in {{Deep Learning}}},
  author = {Lee, Minhyeok},
  year = {2023},
  journal = {Journal of Mathematics},
  volume = {2023},
  number = {1},
  pages = {4229924},
  issn = {2314-4785},
  doi = {10.1155/2023/4229924},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2023/4229924},
  urldate = {2025-08-09},
  abstract = {Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian error linear unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the rectified linear unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. In addition, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide range of deep learning applications. This comprehensive study contributes to a more profound understanding of the underlying mathematical properties of GELU and provides valuable insights for practitioners aiming to select activation functions that optimally align with their specific objectives and constraints in deep learning.},
  copyright = {Copyright © 2023 Minhyeok Lee.},
  langid = {english},
  file = {/Users/sava/Zotero/storage/PLGXHDFV/Lee - 2023 - Mathematical Analysis and Performance Evaluation of the GELU Activation Function in Deep Learning.pdf;/Users/sava/Zotero/storage/3Y8B26U7/4229924.html}
}

@inproceedings{li2017,
  title = {{{DeepCas}}: {{An End-to-end Predictor}} of {{Information Cascades}}},
  shorttitle = {{{DeepCas}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web}}},
  author = {Li, Cheng and Ma, Jiaqi and Guo, Xiaoxiao and Mei, Qiaozhu},
  year = {2017},
  month = apr,
  series = {{{WWW}} '17},
  pages = {577--586},
  publisher = {International World Wide Web Conferences Steering Committee},
  address = {Republic and Canton of Geneva, CHE},
  doi = {10.1145/3038912.3052643},
  url = {https://dl.acm.org/doi/10.1145/3038912.3052643},
  urldate = {2025-08-03},
  abstract = {Information cascades, effectively facilitated by most social network platforms, are recognized as a major factor in almost every social success and disaster in these networks. Can cascades be predicted? While many believe that they are inherently unpredictable, recent work has shown that some key properties of information cascades, such as size, growth, and shape, can be predicted by a machine learning algorithm that combines many features. These predictors all depend on a bag of hand-crafting features to represent the cascade network and the global network structures. Such features, always carefully and sometimes mysteriously designed, are not easy to extend or to generalize to a different platform or domain.Inspired by the recent successes of deep learning in multiple data mining tasks, we investigate whether an end-to-end deep learning approach could effectively predict the future size of cascades. Such a method automatically learns the representation of individual cascade graphs in the context of the global network structure, without hand-crafted features or heuristics. We find that node embeddings fall short of predictive power, and it is critical to learn the representation of a cascade graph as a whole. We present algorithms that learn the representation of cascade graphs in an end-to-end manner, which significantly improve the performance of cascade prediction over strong baselines including feature based methods, node embedding methods, and graph kernel methods. Our results also provide interesting implications for cascade prediction in general.},
  isbn = {978-1-4503-4913-0},
  file = {/Users/sava/Zotero/storage/XXFP9HTA/Li et al. - 2017 - DeepCas An End-to-end Predictor of Information Cascades.pdf}
}

@article{li2024,
  title = {{{GRASS}}: {{Learning Spatial}}–{{Temporal Properties From Chainlike Cascade Data}} for {{Microscopic Diffusion Prediction}}},
  shorttitle = {{{GRASS}}},
  author = {Li, Huacheng and Xia, Chunhe and Wang, Tianbo and Wang, Zhao and Cui, Peng and Li, Xiaojian},
  year = {2024},
  month = nov,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {35},
  number = {11},
  pages = {16313--16327},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2023.3293689},
  url = {https://ieeexplore.ieee.org/abstract/document/10187625/citations},
  urldate = {2025-08-03},
  abstract = {Information diffusion prediction captures diffusion dynamics of online messages in social networks. Thus, it is the basis of many essential tasks such as popularity prediction and viral marketing. However, there are two thorny problems caused by the loss of spatial–temporal properties of cascade data: “position-hopping” and “branch-independency.” The former means no exact propagation relationship between any two consecutive infected users. The latter indicates that not all previously infected users contribute to the prediction of the next infected user. This article proposes the GRU-like Attention Unit and Structural Spreading (GRASS) model for microscopic cascade prediction to overcome the above two problems. First, we introduce the attention mechanism into the gated recurrent unit (GRU) component to expand the restricted receptive field of the recurrent neural network (RNN)-type module, thus addressing the “position-hopping” problem. Second, the structural spreading (SS) mechanism leverages structural features to filter out related users and controls the generation of cascade hidden states, thereby solving the “branch-independency” problem. Experiments on multiple real-world datasets show that our model significantly outperforms state-of-the-art baseline models on both \textbackslash text hits@\textbackslash kappa and \textbackslash text map@\textbackslash kappa metrics. Furthermore, the visualization of latent representations by t-distributed stochastic neighbor embedding (t-SNE) indicates that our model makes different cascades more discriminative during the encoding process.},
  keywords = {Attention,cascade prediction,Mathematical models,Microscopy,neural network,Predictive models,Receivers,Recurrent neural networks,social network analysis,Social networking (online),Trajectory},
  file = {/Users/sava/Zotero/storage/WJV2RGST/citations.html}
}

@misc{liu2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.11692},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2025-08-08},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/sava/Zotero/storage/PE69HNBB/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf;/Users/sava/Zotero/storage/ARCBW2ZB/1907.html}
}

@article{lu2025,
  title = {A fake news detection model using the integration of multimodal attention mechanism and residual convolutional network},
  author = {Lu, Ying and Yao, Naiwei},
  year = {2025},
  month = jul,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {20544},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-05702-w},
  url = {https://www.nature.com/articles/s41598-025-05702-w},
  urldate = {2025-08-28},
  abstract = {To improve the accuracy and efficiency of fake news detection, this study proposes a deep learning model that integrates residual networks with attention mechanisms. Building on traditional convolutional neural networks, the model incorporates multi-head attention mechanisms to enhance the extraction of key features from multimodal data such as text, images, and videos. Additionally, residual connections are introduced to deepen the network architecture, mitigate the vanishing gradient problem, and improve the model’s learning depth and stability. Compared with existing approaches, this study introduces several key innovations. First, it constructs a multimodal feature fusion module that integrates text, image, and video data. Second, it designs a cross-modal alignment mechanism to better connect information across different data types. Third, it optimizes the feature fusion structure for more effective integration. Finally, the study employs attention mechanisms to highlight and enhance the representation of salient features. Experiments were conducted using three representative datasets: the LIAR dataset for political short texts, the FakeNewsNet dataset for English multimodal news, and the Weibo dataset from a Chinese social media platform. These were selected to comprehensively evaluate the model’s performance across different scenarios. Baseline models used for comparison include Bidirectional Encoder Representations from Transformers (BERT), Robustly Optimized Bidirectional Encoder Representations from Transformers Approach (RoBERTa), Generalized Autoregressive Pretraining for Language Understanding (XLNet), Enhanced Representation through Knowledge Integration (ERNIE), and Generative Pre-trained Transformer 3.5 (GPT-3.5). In terms of four key performance metrics—accuracy, precision, recall, and F1 score—the proposed model achieved best-case values of 0.977, 0.986, 0.969, and 0.924, respectively, outperforming the aforementioned baseline models overall. Furthermore, simulated experiments were conducted to evaluate the model’s real-world applicability from four dimensions: robustness, generalization ability, response time, and resource consumption. The results demonstrate that the model maintains strong stability and adaptability under data perturbations and diverse input conditions, with a response time controllable within 0.02~s. The model also shows significant computational advantages when handling large-scale datasets. Therefore, this study presents a high-performance and deployment-friendly solution for fake news detection in multimodal contexts. The study also offers valuable theoretical insights and practical guidance for applying deep learning to public opinion governance and text classification.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Materials science,Mathematics and computing},
  file = {/Users/sava/Zotero/storage/V5CDYT8A/Lu e Yao - 2025 - A fake news detection model using the integration of multimodal attention mechanism and residual con.pdf}
}

@misc{ma2019nlpaug,
  title = {{{NLP}} augmentation},
  author = {{Edward Ma}},
  year = {2019},
  url = {https://github.com/makcedward/nlpaug/tree/master},
  urldate = {2025-08-10},
  file = {/Users/sava/Zotero/storage/SRJWN8AR/master.html}
}

@article{paul2025,
  title = {Meta shelves fact-checking in policy reversal ahead of {{Trump}} administration},
  author = {Paul, Katie and Mukherjee, Supantha and Sophia, Deborah Mary and Mukherjee, Supantha},
  year = {2025},
  month = jan,
  journal = {Reuters},
  url = {https://www.reuters.com/technology/meta-ends-third-party-fact-checking-program-adopts-x-like-community-notes-model-2025-01-07/},
  urldate = {2025-08-22},
  abstract = {The move is Meta's biggest overhaul of its approach to managing political content in recent memory and comes as Mark Zuckerberg has signaled a desire to mend fences with the incoming administration.},
  chapter = {Technology},
  langid = {english},
  file = {/Users/sava/Zotero/storage/2V3RDIYF/meta-ends-third-party-fact-checking-program-adopts-x-like-community-notes-model-2025-01-07.html}
}

@misc{pezet,
  title = {{Occupation de la Gaîté lyrique : y a-t-il une «vague de violences sexuelles», comme l’écrit le «Daily Mail» relayé par Elon Musk ?}},
  shorttitle = {{Occupation de la Gaîté lyrique}},
  author = {Pezet, Jacques},
  journal = {Libération},
  url = {https://www.liberation.fr/checknews/occupation-de-la-gaite-lyrique-y-a-t-il-une-vague-de-violences-sexuelles-comme-lecrit-le-daily-mail-relaye-par-elon-musk-20250304_ZX4PB2BHQBGFVCYPYQGHNUVSWQ/},
  urldate = {2025-08-02},
  abstract = {Un article du tabloïd britannique décrit des violences sexuelles dans la salle de spectacle occupée par de jeunes migrants depuis décembre, sans préciser la source de ses affirmations. La direction du théâtre et ses occupants démentent.},
  chapter = {CheckNews},
  langid = {french},
  file = {/Users/sava/Zotero/storage/CXV5LZTC/occupation-de-la-gaite-lyrique-y-a-t-il-une-vague-de-violences-sexuelles-comme-lecrit-le-daily-.html}
}

@article{ramamoorthy2025,
  title = {Dual stream graph augmented transformer model integrating {{BERT}} and {{GNNs}} for context aware fake news detection},
  author = {Rama Moorthy, Hejamadi and Avinash, N. J. and Krishnaraj Rao, N. S. and Raghunandan, K. R. and Dodmane, Radhakrishna and Blum, Jeremy Joseph and Gabralla, Lubna A.},
  year = {2025},
  month = jul,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {25436},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-05586-w},
  url = {https://www.nature.com/articles/s41598-025-05586-w},
  urldate = {2025-08-02},
  abstract = {The rapid proliferation of misinformation across digital platforms has highlighted the critical need for advanced fake news detection mechanisms. Traditional methods primarily rely on textual analysis, often neglecting the structural patterns of news dissemination, which play a crucial role in determining credibility. To address this limitation, this study proposes a Dual-Stream Graph-Augmented Transformer Model, integrating BERT for deep textual representation and Graph Neural Networks (GNNs) to model the propagation structure of misinformation. The objective is to enhance fake news detection by leveraging both linguistic and network-based features. The proposed method employs Graph Attention Networks (GAT) and Graph Transformers to extract contextual relationships, while an attention-based fusion mechanism effectively integrates textual and graph embeddings for classification. The model is implemented using PyTorch and Hugging Face Transformers, with experiments conducted on the FakeNewsNet dataset, which includes news articles, user interactions, and source metadata. Evaluation metrics such as accuracy, precision, recall, F1-score, and AUC-ROC indicate superior performance, with an accuracy of 99\%, outperforming baseline models such as Bi-LSTM and RoBERTa-GCN. The study concludes that incorporating graph-based propagation features significantly improves fake news detection, providing a robust, scalable, and context-aware solution. Future enhancements will focus on refining credibility assessment mechanisms and extending the model to support multilingual and multimodal misinformation detection across diverse digital platforms.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Engineering,Environmental social sciences,Mathematics and computing},
  file = {/Users/sava/Zotero/storage/6GS4NEG4/Rama Moorthy et al. - 2025 - Dual stream graph augmented transformer model integrating BERT and GNNs for context aware fake news.pdf}
}

@inproceedings{ruchansky2017,
  title = {{{CSI}}: {{A Hybrid Deep Model}} for {{Fake News Detection}}},
  shorttitle = {{{CSI}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Ruchansky, Natali and Seo, Sungyong and Liu, Yan},
  year = {2017},
  month = nov,
  series = {{{CIKM}} '17},
  pages = {797--806},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3132847.3132877},
  url = {https://dl.acm.org/doi/10.1145/3132847.3132877},
  urldate = {2025-08-02},
  abstract = {The topic of fake news has drawn attention both from the public and the academic communities. Such misinformation has the potential of affecting public opinion, providing an opportunity for malicious parties to manipulate the outcomes of public events such as elections. Because such high stakes are at play, automatically detecting fake news is an important, yet challenging problem that is not yet well understood. Nevertheless, there are three generally agreed upon characteristics of fake news: the text of an article, the user response it receives, and the source users promoting it. Existing work has largely focused on tailoring solutions to one particular characteristic which has limited their success and generality.In this work, we propose a model that combines all three characteristics for a more accurate and automated prediction. Specifically, we incorporate the behavior of both parties, users and articles, and the group behavior of users who propagate fake news. Motivated by the three characteristics, we propose a model called CSI which is composed of three modules: Capture, Score, and Integrate. The first module is based on the response and text; it uses a Recurrent Neural Network to capture the temporal pattern of user activity on a given article. The second module learns the source characteristic based on the behavior of users, and the two are integrated with the third module to classify an article as fake or not. Experimental analysis on real-world data demonstrates that CSI achieves higher accuracy than existing models, and extracts meaningful latent representations of both users and articles.},
  isbn = {978-1-4503-4918-5},
  file = {/Users/sava/Zotero/storage/KFMLU7UD/Ruchansky et al. - 2017 - CSI A Hybrid Deep Model for Fake News Detection.pdf}
}

@inproceedings{shu2019,
  title = {{{dEFEND}}: {{Explainable Fake News Detection}}},
  shorttitle = {{{dEFEND}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Shu, Kai and Cui, Limeng and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {395--405},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3292500.3330935},
  url = {https://dl.acm.org/doi/10.1145/3292500.3330935},
  urldate = {2025-08-02},
  abstract = {In recent years, to mitigate the problem of fake news, computational detection of fake news has been studied, producing some promising early results. While important, however, we argue that a critical missing piece of the study be the explainability of such detection, i.e., why a particular piece of news is detected as fake. In this paper, therefore, we study the explainable detection of fake news. We develop a sentence-comment co-attention sub-network to exploit both news contents and user comments to jointly capture explainable top-k check-worthy sentences and user comments for fake news detection. We conduct extensive experiments on real-world datasets and demonstrate that the proposed method not only significantly outperforms 7 state-of-the-art fake news detection methods by at least 5.33\% in F1-score, but also (concurrently) identifies top-k user comments that explain why a news piece is fake, better than baselines by 28.2\% in NDCG and 30.7\% in Precision.},
  isbn = {978-1-4503-6201-6},
  file = {/Users/sava/Zotero/storage/UF7PRXLT/Shu et al. - 2019 - dEFEND Explainable Fake News Detection.pdf}
}

@article{shu2020,
  title = {{{FakeNewsNet}}: {{A Data Repository}} with {{News Content}}, {{Social Context}}, and {{Spatiotemporal Information}} for {{Studying Fake News}} on {{Social Media}}},
  shorttitle = {{{FakeNewsNet}}},
  author = {Shu, Kai and Mahudeswaran, Deepak and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  year = {2020},
  month = jun,
  journal = {Big Data},
  volume = {8},
  number = {3},
  pages = {171--188},
  publisher = {Mary Ann Liebert, Inc., publishers},
  issn = {2167-6461},
  doi = {10.1089/big.2020.0062},
  url = {https://www.liebertpub.com/doi/10.1089/big.2020.0062},
  urldate = {2025-08-03},
  abstract = {Social media has become a popular means for people to consume and share the news. At the same time, however, it has also enabled the wide dissemination of fake news, that is, news with intentionally false information, causing significant negative effects on society. To mitigate this problem, the research of fake news detection has recently received a lot of attention. Despite several existing computational solutions on the detection of fake news, the lack of comprehensive and community-driven fake news data sets has become one of major roadblocks. Not only existing data sets are scarce, they do not contain a myriad of features often required in the study such as news content, social context, and spatiotemporal information. Therefore, in this article, to facilitate fake news-related research, we present a fake news data repository FakeNewsNet, which contains two comprehensive data sets with diverse features in news content, social context, and spatiotemporal information. We present a comprehensive description of the FakeNewsNet, demonstrate an exploratory analysis of two data sets from different perspectives, and discuss the benefits of the FakeNewsNet for potential applications on fake news study on social media.}
}

@misc{tf12024,
  title = {{"Psychose des punaises de lit en France" : pourquoi le gouvernement accuse-t-il la Russie ?}},
  shorttitle = {{"Psychose des punaises de lit en France"}},
  year = {2024},
  month = mar,
  journal = {TF1 INFO},
  url = {https://www.tf1info.fr/international/psychose-des-punaises-de-lit-en-france-pourquoi-le-ministre-delegue-charge-de-l-europe-jean-noel-barrot-accuse-t-il-la-russie-doopelganger-2287905.html},
  urldate = {2025-08-30},
  abstract = {[VIDÉO] Le ministre délégué à l'Europe, Jean-Noël Barrot, était l'invité de la matinale de TF1 ce vendredi. Sur notre plateau, il a affirmé que la psychose liée aux punaises de lit en France a été amplifiée par la Russie. L'occasion de revenir sur les preuves de cette ingérence numérique. - "Psychose des punaises de lit en France"~: pourquoi le gouvernement accuse-t-il la Russie ? (International).},
  chapter = {International},
  langid = {french},
  file = {/Users/sava/Zotero/storage/YRSGR2H6/psychose-des-punaises-de-lit-en-france-pourquoi-le-ministre-delegue-charge-de-l-europe-jean-noe.html}
}

@article{vosoughi2018,
  title = {The spread of true and false news online},
  author = {Vosoughi, Soroush and Roy, Deb and Aral, Sinan},
  year = {2018},
  month = mar,
  journal = {Science},
  volume = {359},
  number = {6380},
  pages = {1146--1151},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aap9559},
  url = {https://www.science.org/doi/10.1126/science.aap9559},
  urldate = {2025-08-19},
  abstract = {We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise \textasciitilde 126,000 stories tweeted by \textasciitilde 3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98\% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.}
}

@article{zhao2022,
  title = {Predicting information diffusion via deep temporal convolutional networks},
  author = {Zhao, Qihang and Zhang, Yuzhe and Feng, Xiaodong},
  year = {2022},
  month = sep,
  journal = {Information Systems},
  volume = {108},
  pages = {102045},
  issn = {0306-4379},
  doi = {10.1016/j.is.2022.102045},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437922000412},
  urldate = {2025-08-03},
  abstract = {Information cascade diffusion is ubiquitous in modern social medias and other fields, such as viral marketing, paper citation dynamics, and public opinion communication. However, the existing deep learning-based methods to model and predict the growth of information cascades pay much attention to the nodes in the cascades and ignore the overall propagation structure of the cascades. Simultaneously, they are usually of high complexity and low computational efficiency. In this paper, we propose a novel deep learning framework for information cascade predictor, named CasTCN, which can effectively capture the structure dynamic of the information cascades. Firstly, we design a dynamic mapping mechanism, which can represent the overall structure of information cascades an its dynamic evolution. Secondly, we extract the higher-level representation of cascade network through deep temporal convolutional network. Finally, the prediction module is applied to predict the growth scale of information cascades. Since CasTCN is a graph-level method, it has relatively fewer model parameters, so the model training time is also less than other baseline methods. Our experiments on two real-world datasets show that CasTCN can achieve better performance than other baseline methods on both effectiveness and efficiency. Compared with the state-of-the-art methods, the prediction error is reduced by 8.58\%, 7.13\% and 6.89\% respectively on 3 subdatasets of Weibo, and 15.00\%, 11.02\% and 9.12\% on 3 subdatasets of APS respectively. Also, the training time is much more less than baselines.},
  keywords = {Dynamic mapping,Information diffusion,Social networks,Temporal convolutional networks},
  file = {/Users/sava/Zotero/storage/3ZP4A528/S0306437922000412.html}
}
